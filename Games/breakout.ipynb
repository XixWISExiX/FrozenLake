{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Installing and importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides a stable environment for breakout\n",
    "# !pip install stable-baselines[mpi]==2.8.0\n",
    "# Download and install ROMs (makes Gymnasium run breakout)\n",
    "# !pip install gymnasuim[all,accept-rom-license]\n",
    "# !gdown -q http://www.atarimania.com/roms/Roms.rar\n",
    "# !pip install -q unrar\n",
    "# !mkdir ./roms_atari\n",
    "# !unrar x Roms.rar ./roms_atari > /dev/null 2>&1\n",
    "# !python -m atari_py.import_roms ./roms_atari > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !pip install tensorflow\n",
    "# !pip install matplotlib\n",
    "# !pip install gymnasium[atari]\n",
    "# !pip install gymnasium[accept-rom-license]\n",
    "# ---- !pip install opencv-python\n",
    "# !pip install scipy\n",
    "# ---- !pip install git+https://github.com/openai/baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Examining the Breakout environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "env = gymnasium.make('ALE/Breakout-v5')\n",
    "obs = env.reset()\n",
    "# print(obs[0])\n",
    "obs_arr = np.array(obs[0])\n",
    "plt.imshow(obs_arr)\n",
    "plt.title(\"Breakout Visualization\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (2, 210, 160) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m next_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_state[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m arr \u001b[38;5;241m=\u001b[39m [grayscale_array, next_arr]\n\u001b[0;32m---> 14\u001b[0m npy \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# arr.append(grayscale_array)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(obs[0])\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# obs_arr = np.array(obs[0])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# trimmed_array = arr[30:-16, 7:-7, :]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(trimmed_array.shape)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(npy\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (2, 210, 160) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make('ALE/Breakout-v5')\n",
    "obs = env.reset()\n",
    "rgb_array = np.array(env.reset()[0])\n",
    "grayscale_array = np.mean(rgb_array, axis=-1, keepdims=True)\n",
    "# Take an action in the environment\n",
    "action = env.action_space.sample()  # Replace this with your desired action\n",
    "# next_state, reward, done, info = env.step(action)\n",
    "next_state = env.step(action)\n",
    "next_arr = np.array(next_state[0])\n",
    "arr = [grayscale_array, next_arr]\n",
    "npy = np.array(arr)\n",
    "# arr.append(grayscale_array)\n",
    "# print(obs[0])\n",
    "# obs_arr = np.array(obs[0])\n",
    "# trimmed_array = arr[30:-16, 7:-7, :]\n",
    "# print(trimmed_array.shape)\n",
    "print(npy.shape)\n",
    "# plt.imshow(next_state[0])\n",
    "# plt.imshow(trimmed_array)\n",
    "plt.imshow(npy)\n",
    "plt.title(\"Breakout Visualization\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "env = gymnasium.make('ALE/Breakout-v5')\n",
    "wrapped_env = RescaleAction(env, min_action=0, max_action=1)\n",
    "# env = gymnasium.make(\"BreakoutNoFrameskip-v4\", env_id=\"ALE/Breakout-v5\")\n",
    "# env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "rgb_array = np.array(env.reset()[0])\n",
    "# print(obs.shape)\n",
    "grayscale_array = np.mean(rgb_array, axis=-1, keepdims=True)\n",
    "# obs = rgb_array \n",
    "obs = grayscale_array\n",
    "# TODO try and make the preprocessing by deleting the header and downsize the image to 84x84, (noralize brightness), stack 4 consecutave frames\n",
    "# trimmed_array = grayscale_array[:, 7:-7, :]\n",
    "print(env.observation_space.shape[0])\n",
    "# obs = trimmed_array\n",
    "print(obs.shape)\n",
    "\n",
    "plt.title(\"Agent Observation (4 frames)\")\n",
    "plt.imshow(obs.transpose([0, 2, 1]).reshape([env.observation_space.shape[0], -1]))\n",
    "# plt.imshow(obs)\n",
    "# resized_array = zoom(grayscale_array, zoom=(84/210, 84/160, 4), order=1)\n",
    "\n",
    "# print(grayscale_array.shape)\n",
    "# # obs = rgb_array \n",
    "# # obs = grayscale_array\n",
    "# obs = resized_array \n",
    "# print(obs.shape)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# # env = resized_array\n",
    "# plt.title(\"Agent Observation (4 frames)\")\n",
    "# # plt.imshow(obs.transpose([0, 2, 1]).reshape([env.observation_space.shape[0], -1]))\n",
    "# plt.imshow(resized_array[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Visualize the breakout environment\n",
    "env = gymnasium.make('ALE/Breakout-v5', render_mode='human')\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width, channels = env.observation_space.shape\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99 # Discount factor for past rewards\n",
    "epsilon = 1.0 # Epsilon greedy parameter \n",
    "epsilon_min = 0.1 # Minimum epsilon greedy paramter\n",
    "epsilon_max = 1.0 # Maximum epsilon greedy paramter\n",
    "\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ") # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32 # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating a model - Deep convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 4\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 4,))\n",
    "\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to make an action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Configuring the paramaters and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer which improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluating and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:  # Run until solved\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Making videos of our agent play"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
